{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emulators: Measuring performance\n",
    "\n",
    "This example illustrates how a neural networks performs in emulating the log-likelihood surface of a time series and in Bayesian inference, using a two-step MCMC procedure with emulator neural networks [Emulated Metropolis MCMC](../sampling/first-example.ipynb).\n",
    "\n",
    "It follows on from [Emulators: First example](../mcmc/first-example-emulator.ipynb)\n",
    "\n",
    "Like in the first example, I start by importing pints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Emulator'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-b7dbe29b08e1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpints\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\plagl\\pints\\pints\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[1;31m#  Emulators\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 245\u001b[1;33m from ._emulators import (\n\u001b[0m\u001b[0;32m    246\u001b[0m     \u001b[0mEmulator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m     \u001b[0mNNEmulator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'Emulator'"
     ]
    }
   ],
   "source": [
    "import pints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I create a model class using the \"Logistic\" toy model included in pints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pints.toy as toy\n",
    "\n",
    "class RescaledModel(pints.ForwardModel):\n",
    "    def __init__(self):\n",
    "        self.base_model = toy.LogisticModel()\n",
    "    \n",
    "    def simulate(self, parameters, times):\n",
    "        # Run a simulation with the given parameters for the\n",
    "        # given times and return the simulated values\n",
    "        r, k = parameters\n",
    "        r = r / 50\n",
    "        k = k * 500\n",
    "        return self.base_model.simulate([r, k], times)\n",
    "    \n",
    "    def simulateS1(self, parameters, times):\n",
    "        # Run a simulation with the given parameters for the\n",
    "        # given times and return the simulated values\n",
    "        r, k = parameters\n",
    "        r = r / 50\n",
    "        k = k * 500\n",
    "        return self.base_model.simulateS1([r, k], times)\n",
    "    \n",
    "    def n_parameters(self):\n",
    "        # Return the dimension of the parameter vector\n",
    "        return 2\n",
    "    \n",
    "model = toy.LogisticModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to generate some test data, I choose an arbitrary set of \"true\" parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_parameters = [0.015, 500]\n",
    "start_parameters = [0.75, 1.0] # rescaled true parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And a number of time points at which to sample the time series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "times = np.linspace(0, 1000, 400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using these parameters and time points, I generate an example dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org_values = model.simulate(true_parameters, times)\n",
    "range_values = max(org_values) - min(org_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And make it more realistic by adding gaussian noise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = 0.05 * range_values\n",
    "print(\"Gaussian noise:\", noise)\n",
    "values = org_values + np.random.normal(0, noise, org_values.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using matplotlib and seaborn (optional - for styling), I look at the noisy time series I just simulated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "\n",
    "sns.set(context='notebook', style='whitegrid', palette=\"deep\", font='Times New Roman', \n",
    "        font_scale=1.5, color_codes=True, rc={\"grid.linewidth\": 1})\n",
    "palette = itertools.cycle(sns.color_palette())\n",
    "c=next(palette)\n",
    "\n",
    "fig = plt.figure(figsize=(12,4.5))\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Values')\n",
    "plt.plot(times, org_values, lw=2, c=c, label='Original data')\n",
    "plt.plot(times, values, '--', c=c, label='Noisy data')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "fig.savefig(\"results/logistic.png\", bbox_inches='tight', dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I have enough data (a model, a list of times, and a list of values) to formulate a PINTS problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RescaledModel()\n",
    "problem = pints.SingleOutputProblem(model, times, values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I now have some toy data, and a model that can be used for forward simulations. To make it into a probabilistic problem, a _noise model_ needs to be added. This can be done using the `GaussianLogLikelihood` function, which assumes independently distributed Gaussian noise over the data, and can calculate log-likelihoods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_likelihood = pints.GaussianKnownSigmaLogLikelihood(problem, noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This `log_likelihood` represents the _conditional probability_ $p(y|\\theta)$, given a set of parameters $\\theta$ and a series of $y=$ `values`, it can calculate the probability of finding those values if the real parameters are $\\theta$.\n",
    "\n",
    "This can be used in a Bayesian inference scheme to find the quantity of interest:\n",
    "\n",
    "$p(\\theta|y) = \\frac{p(\\theta)p(y|\\theta)}{p(y)} \\propto p(\\theta)p(y|\\theta)$\n",
    "\n",
    "To solve this, a _prior_ is defined, indicating an initial guess about what the parameters should be. \n",
    "Similarly as using a _log-likelihood_ (the natural logarithm of a likelihood), this is defined by using a _log-prior_. Hence, the above equation simplifies to:\n",
    "\n",
    "$\\log p(\\theta|y) \\propto \\log p(\\theta) + \\log p(y|\\theta)$\n",
    "\n",
    "In this example, it is assumed that we don't know too much about the prior except lower and upper bounds for each variable: We assume the first model parameter is somewhere on the interval $[0.01, 0.02]$, the second model parameter on $[400, 600]$, and the standard deviation of the noise is somewhere on $[1, 100]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create (rescaled) bounds for our parameters and get prior\n",
    "#bounds = pints.RectangularBoundaries([0.5, 0.8], [1.0, 1.2])\n",
    "#bounds = pints.RectangularBoundaries([0.7125, 0.95], [0.7875, 1.05])\n",
    "#bounds = pints.RectangularBoundaries([0.675, 0.90], [0.825, 1.1])\n",
    "#bounds = pints.RectangularBoundaries([0.525, 0.7], [0.975, 1.3])\n",
    "bounds = pints.RectangularBoundaries([0.6, 0.8], [0.9, 1.2])\n",
    "log_prior = pints.UniformLogPrior(bounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this prior, the numerator of Bayes' rule can be defined -- the unnormalised log posterior, $\\log \\left[ p(y|\\theta) p(\\theta) \\right]$, which is the natural logarithm of the likelihood times the prior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a posterior log-likelihood (log(likelihood * prior))\n",
    "log_posterior = pints.LogPosterior(log_likelihood, log_prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D \n",
    "\n",
    "sns.set(context='notebook', style='whitegrid', palette=\"deep\", font='Times New Roman', \n",
    "        font_scale=1.5, color_codes=True, rc={\"grid.linewidth\": 1})\n",
    "\n",
    "input_parameters = log_prior.sample(2000)\n",
    "x = [p[0] for p in input_parameters]\n",
    "y = [p[1] for p in input_parameters]\n",
    "likelihoods = np.apply_along_axis(log_likelihood, 1, input_parameters)\n",
    "likelihoods[:5]\n",
    "\n",
    "print(min(x), max(x))\n",
    "print(min(y), max(y))\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(x, y, list(likelihoods))\n",
    "plt.show()\n",
    "#fig.savefig(\"figures/training-data-best-nn-6-64.png\", bbox_inches='tight', dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(input_parameters, likelihoods, test_size=0.3, random_state=0)\n",
    "emu = pints.MultiLayerNN(problem, X_train, y_train, input_scaler=MinMaxScaler(), output_scaler=StandardScaler())\n",
    "emu.set_parameters(layers=6, neurons=64, hidden_activation='relu', activation='linear', learning_rate=0.0001)\n",
    "hist = emu.fit(epochs=500, batch_size=32, X_val=X_valid, y_val=y_valid, verbose=0)\n",
    "emu.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emu([0.75, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_likelihood([0.75, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "#print(hist.history.keys())\n",
    "sns.set(context='notebook', style='whitegrid', palette=\"deep\", font='Times New Roman', \n",
    "        font_scale=1.5, color_codes=True, rc={\"grid.linewidth\": 1})\n",
    "\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(20,10))                 \n",
    "ax1.title.set_text('Learning curves based on MSE')\n",
    "ax2.title.set_text('Learning curves based on MAE')\n",
    "\n",
    "ax1.plot(hist.history['loss'])\n",
    "ax1.plot(hist.history['val_loss'])\n",
    "ax1.set_ylabel('MSE')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.legend(['training', 'validation'], loc='upper left')\n",
    "\n",
    "ax2.plot(hist.history['mean_absolute_error'])\n",
    "ax2.plot(hist.history['val_mean_absolute_error'])\n",
    "ax2.set_ylabel('MAE')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.legend(['training', 'validation'], loc='upper left')\n",
    "\n",
    "ax3.plot(hist.history['rescaled_mse'])\n",
    "ax3.plot(hist.history['val_rescaled_mse'])\n",
    "ax3.set_ylabel('Rescaled MSE')\n",
    "ax3.set_xlabel('Epoch')\n",
    "ax3.legend(['training', 'validation'], loc='upper left')\n",
    "\n",
    "ax4.plot(hist.history['rescaled_mae'])\n",
    "ax4.plot(hist.history['val_rescaled_mae'])\n",
    "ax4.set_ylabel('Rescaled MAE')\n",
    "ax4.set_xlabel('Epoch')\n",
    "ax4.legend(['training', 'validation'], loc='upper left')\n",
    "\n",
    "plt.show()\n",
    "fig.savefig(\"results/training-best-nn-6-64.png\", bbox_inches='tight', dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(hist.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from mpl_toolkits.mplot3d import Axes3D \n",
    "\n",
    "sns.set(context='notebook', style='whitegrid', palette=\"deep\", font='Times New Roman', \n",
    "        font_scale=1.5, color_codes=True, rc={\"grid.linewidth\": 1})\n",
    "\n",
    "test_splits = 50 # number of splits along each axis\n",
    "r_grid, k_grid, test_data = pints.generate_grid(bounds.lower(), bounds.upper(), test_splits)\n",
    "model_prediction = pints.predict_grid(log_likelihood, test_data)\n",
    "emu_prediction = pints.predict_grid(emu, test_data)\n",
    "\n",
    "angle=(25, 300)\n",
    "alpha=0.7\n",
    "fontsize=16\n",
    "labelpad=15\n",
    "\n",
    "title = \"Comparison of log-likelihood surfaces\"\n",
    "x_label = \"Growth rate (r)\"\n",
    "y_label = \"Carrying capacity (k)\"\n",
    "z_label = \"Log-likelihood\"\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.plot_surface(r_grid, k_grid, model_prediction, cmap='Blues', edgecolor='none', alpha=alpha)\n",
    "ax.plot_surface(r_grid, k_grid, emu_prediction, cmap='Oranges', edgecolor='none', alpha=alpha)\n",
    "ax.view_init(*angle)\n",
    "\n",
    "plt.title(title, fontsize=fontsize*1.25)\n",
    "ax.set_xlabel(x_label, fontsize=fontsize, labelpad=labelpad)\n",
    "ax.set_ylabel(y_label, fontsize=fontsize, labelpad=labelpad)\n",
    "ax.set_zlabel(z_label, fontsize=fontsize, labelpad=labelpad)\n",
    "\n",
    "fake2Dline1 = mpl.lines.Line2D([0],[0], linestyle=\"none\", c='blue', marker = 'o', alpha=0.5)\n",
    "fake2Dline2 = mpl.lines.Line2D([0],[0], linestyle=\"none\", c='orange', marker = 'o', alpha=0.8)\n",
    "ax.legend([fake2Dline1, fake2Dline2], [\"True log-likelihood\", \"NN emulator log-likelihood\"])\n",
    "plt.show()\n",
    "fig.savefig(\"results/likelihood-surfaces-best-nn-6-64.png\", bbox_inches='tight', dpi=200)\n",
    "\n",
    "mape = np.mean(np.abs((model_prediction - emu_prediction) / model_prediction))\n",
    "mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_k = np.array([0.0, 0.005])\n",
    "delta_r = np.array([0.005, 0.0])\n",
    "gradients_k = (pints.predict_grid(log_likelihood, (test_data + delta_k)) - pints.predict_grid(log_likelihood, (test_data - delta_k))) / (sum(2*delta_k))\n",
    "gradients_r = (pints.predict_grid(log_likelihood, (test_data + delta_r)) - pints.predict_grid(log_likelihood, (test_data - delta_r))) / (sum(2*delta_r)) \n",
    "emu_gradients_k = (pints.predict_grid(emu, (test_data + delta_k)) - pints.predict_grid(emu, (test_data - delta_k))) / (sum(2*delta_k))\n",
    "emu_gradients_r = (pints.predict_grid(emu, (test_data + delta_r)) - pints.predict_grid(emu, (test_data - delta_r))) / (sum(2*delta_r)) \n",
    "mape_k = np.mean(np.abs((gradients_k - emu_gradients_k) / gradients_k)) \n",
    "mape_r = np.mean(np.abs((gradients_r - emu_gradients_r) / gradients_r))  \n",
    "print((mape_k+mape_r)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "n_chains = 3\n",
    "n_iter = 30000 # Add stopping criterion\n",
    "warm_up = 10000\n",
    "sigma0 = np.abs(start_parameters) * 5e-05 # Choose a covariance matrix for the proposal step\n",
    "x0 = [\n",
    "    np.array(start_parameters) * 0.9,#0.95,#0.9,\n",
    "    np.array(start_parameters) * 1.05,#0.97,#1.05,\n",
    "    np.array(start_parameters) * 1.15,#1.05,#1.15,\n",
    "]\n",
    "scaling_factors = [1/50, 500]\n",
    "param_names=[\"r\", \"k\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_posterior_emu = pints.LogPosterior(emu, log_prior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running MCMC routines\n",
    "\n",
    "### Adaptive Covariance MCMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mcmc routine\n",
    "mcmc = pints.MCMCController(log_posterior, n_chains, x0)\n",
    "\n",
    "# Add stopping criterion\n",
    "mcmc.set_max_iterations(n_iter)\n",
    "\n",
    "# Disable logging mode\n",
    "#mcmc.set_log_to_screen(False)\n",
    "\n",
    "# Run!\n",
    "print('Running...')\n",
    "chains = mcmc.run()\n",
    "print('Done!')\n",
    "\n",
    "# Show traces and histograms\n",
    "pints.plot.trace(chains)\n",
    "\n",
    "# Discard warm up\n",
    "chains_thinned = chains[:, warm_up:, :]\n",
    "\n",
    "# Check convergence using rhat criterion\n",
    "print('R-hat:')\n",
    "print(pints.rhat_all_params(chains_thinned))\n",
    "\n",
    "# Look at distribution in chain 0\n",
    "pints.plot.pairwise(chains_thinned[0])\n",
    "\n",
    "# Show graphs\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Metropolis Hastings MCMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mcmc routine\n",
    "mcmc = pints.MCMCController(log_posterior, n_chains, x0, sigma0=sigma0, method=pints.MetropolisRandomWalkMCMC)\n",
    "\n",
    "# Add stopping criterion\n",
    "mcmc.set_max_iterations(n_iter)\n",
    "\n",
    "# Disable logging mode\n",
    "#mcmc.set_log_to_screen(False)\n",
    "\n",
    "# Run!\n",
    "print('Running...')\n",
    "metropolis_chains = mcmc.run()\n",
    "print('Done!')\n",
    "\n",
    "# Show traces and histograms\n",
    "pints.plot.trace(metropolis_chains)\n",
    "\n",
    "# Discard warm up\n",
    "metropolis_chains_thinned = metropolis_chains[:, warm_up:, :]\n",
    "\n",
    "# Check convergence using rhat criterion\n",
    "print('R-hat:')\n",
    "print(pints.rhat_all_params(metropolis_chains_thinned))\n",
    "\n",
    "# Look at distribution in chain 0\n",
    "pints.plot.pairwise(metropolis_chains_thinned[0])\n",
    "\n",
    "# Show graphs\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revert scaling\n",
    "metropolis_chains_rescaled = np.copy(metropolis_chains)\n",
    "metropolis_chain_rescaled = metropolis_chains_rescaled[0]\n",
    "metropolis_chain_rescaled = metropolis_chain_rescaled[warm_up:]\n",
    "metropolis_chains = np.array([[[s*f for s,f in zip(samples, scaling_factors)] for samples in chain] \n",
    "                              for chain in metropolis_chains])\n",
    "metropolis_chain = metropolis_chains[0]\n",
    "metropolis_chain = metropolis_chain[warm_up:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metropolis Hastings MCMC using NN as posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mcmc routine\n",
    "mcmc = pints.MCMCController(log_posterior_emu, n_chains, x0, sigma0=sigma0, method=pints.MetropolisRandomWalkMCMC)\n",
    "\n",
    "# Add stopping criterion\n",
    "mcmc.set_max_iterations(n_iter)\n",
    "\n",
    "# Disable logging mode\n",
    "#mcmc.set_log_to_screen(False)\n",
    "\n",
    "# Run!\n",
    "print('Running...')\n",
    "chains_emu = mcmc.run()\n",
    "print('Done!')\n",
    "\n",
    "# Show traces and histograms\n",
    "pints.plot.trace(chains_emu)\n",
    "\n",
    "# Discard warm up\n",
    "chains_emu_thinned = chains_emu[:, warm_up:, :]\n",
    "\n",
    "# Check convergence using rhat criterion\n",
    "print('R-hat:')\n",
    "print(pints.rhat_all_params(chains_emu_thinned))\n",
    "\n",
    "# Look at distribution in chain 0\n",
    "pints.plot.pairwise(chains_emu_thinned[0])\n",
    "\n",
    "# Show graphs\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revert scaling\n",
    "chains_emu_rescaled = np.copy(chains_emu)\n",
    "chain_emu_rescaled = chains_emu_rescaled[0]\n",
    "chain_emu_rescaled = chain_emu_rescaled[warm_up:]\n",
    "chains_emu = np.array([[[s*f for s,f in zip(samples, scaling_factors)] for samples in chain] for chain in chains_emu])\n",
    "chain_emu = chains_emu[0]\n",
    "chain_emu = chain_emu[warm_up:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-Step MCMC using NN as emulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mcmc routine\n",
    "mcmc = pints.MCMCController(log_posterior_emu, n_chains, x0, sigma0=sigma0, method=pints.EmulatedMetropolisMCMC, f=log_posterior)\n",
    "\n",
    "# Add stopping criterion\n",
    "mcmc.set_max_iterations(n_iter)\n",
    "\n",
    "# Disable logging mode\n",
    "#mcmc.set_log_to_screen(False)\n",
    "\n",
    "# Run!\n",
    "print('Running...')\n",
    "emulated_chains = mcmc.run()\n",
    "print('Done!')\n",
    "\n",
    "# Show traces and histograms\n",
    "pints.plot.trace(emulated_chains)\n",
    "\n",
    "# Discard warm up\n",
    "emulated_chains_thinned = emulated_chains[:, warm_up:, :]\n",
    "\n",
    "# Check convergence using rhat criterion\n",
    "print('R-hat:')\n",
    "print(pints.rhat_all_params(emulated_chains_thinned))\n",
    "\n",
    "# Look at distribution in chain 0\n",
    "pints.plot.pairwise(emulated_chains_thinned[0])\n",
    "\n",
    "# Show graphs\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acceptance_rates = mcmc.acceptance_rates()\n",
    "acceptance_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revert scaling\n",
    "emulated_chains_rescaled = np.copy(emulated_chains)\n",
    "emulated_chain_rescaled = emulated_chains_rescaled[0]\n",
    "emulated_chain_rescaled = emulated_chain_rescaled[warm_up:]\n",
    "emulated_chains = np.array([[[s*f for s,f in zip(samples, scaling_factors)] for samples in chain] \n",
    "                            for chain in emulated_chains])\n",
    "emulated_chain = emulated_chains[0]\n",
    "emulated_chain = emulated_chain[warm_up:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining NN performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emu_prediction = np.apply_along_axis(emu, 1, metropolis_chain_rescaled).flatten()\n",
    "model_prediction = np.apply_along_axis(log_likelihood, 1, metropolis_chain_rescaled).flatten()\n",
    "diffs = (np.abs((model_prediction - emu_prediction) / model_prediction))\n",
    "\n",
    "iters = np.linspace(0, n_iter-warm_up, len(metropolis_chain_rescaled))\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.title(\"Emulator and model absolute differences along a chain of MCMC\")\n",
    "plt.xlabel(\"Number of iterations\")\n",
    "plt.ylabel(\"Likelihood\")\n",
    "plt.plot(iters, diffs, color = \"Black\")\n",
    "plt.show()\n",
    "\n",
    "diffs.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emu_prediction = np.apply_along_axis(emu, 1, chain_emu_rescaled).flatten()\n",
    "model_prediction = np.apply_along_axis(log_likelihood, 1, metropolis_chain_rescaled).flatten()\n",
    "diffs = (np.abs((model_prediction - emu_prediction) / model_prediction))\n",
    "\n",
    "iters = np.linspace(0, n_iter-warm_up, len(chain_emu_rescaled))\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "plt.title(\"Emulator and model errors along a chain of MCMC\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Absolute percentage error\")\n",
    "plt.plot(iters, diffs)#, color = \"Black\")\n",
    "plt.show()\n",
    "fig.savefig(\"results/mcmc-diffs-best-nn-6-64.png\", bbox_inches='tight', dpi=200)\n",
    "\n",
    "diffs[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(context='notebook', style='ticks', palette=\"deep\", font='Times New Roman', \n",
    "        font_scale=1.5, color_codes=True, rc={\"grid.linewidth\": 1})\n",
    "\n",
    "# Create grid of parameters\n",
    "x = [p[0] for p in metropolis_chain_rescaled]\n",
    "y = [p[1] for p in metropolis_chain_rescaled]\n",
    "xmin, xmax = np.min(x), np.max(x)\n",
    "ymin, ymax = np.min(y), np.max(y)\n",
    "xx, yy = np.mgrid[xmin:xmax:100j, ymin:ymax:100j]\n",
    "params = [list(n) for n in zip(xx, yy)]\n",
    "ll = np.apply_along_axis(log_likelihood, 1, params)\n",
    "ll_emu = np.apply_along_axis(emu, 1, params)\n",
    "ll_emu = [list(e[0][0]) for e in ll_emu]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,7))\n",
    "ax1.title.set_text('Log-Likelihood')\n",
    "ax2.title.set_text('Neural Network')\n",
    "ax1.set_xlabel('Rescaled growth rate (r)')\n",
    "ax1.set_ylabel('Rescaled carrying capacity (k)')\n",
    "ax2.set_xlabel('Rescaled growth rate (r)')\n",
    "ax2.set_ylabel('Rescaled carrying capacity (k)')\n",
    "\n",
    "ax1.contourf(xx, yy, ll, cmap='Blues', alpha=0.8, extent=[xmin, xmax, ymin, ymax])\n",
    "ax1.contour(xx, yy, ll, colors='k')\n",
    "ax2.contourf(xx, yy, ll_emu, cmap='Oranges', alpha=0.8, extent=[xmin, xmax, ymin, ymax])\n",
    "ax2.contour(xx, yy, ll_emu, colors='k')\n",
    "plt.show()\n",
    "fig.savefig(\"results/close-contours-best-nn-6-64.png\", bbox_inches='tight', dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set(context='notebook', style='ticks', palette=\"deep\", font='Times New Roman', \n",
    "        font_scale=1.5, color_codes=True, rc={\"grid.linewidth\": 1})\n",
    "\n",
    "# Create grid of parameters\n",
    "xmin, xmax = 0.5, 1.0\n",
    "ymin, ymax = 0.8, 1.2\n",
    "xx, yy = np.mgrid[xmin:xmax:100j, ymin:ymax:100j]\n",
    "params = [list(n) for n in zip(xx, yy)]\n",
    "ll = np.apply_along_axis(log_likelihood, 1, params)\n",
    "ll_emu = np.apply_along_axis(emu, 1, params)\n",
    "ll_emu = [list(e[0][0]) for e in ll_emu]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,7))\n",
    "ax1.title.set_text('Log-Likelihood')\n",
    "ax2.title.set_text('Neural Network')\n",
    "ax1.set_xlabel('Rescaled growth rate (r)')\n",
    "ax1.set_ylabel('Rescaled carrying capacity (k)')\n",
    "ax2.set_xlabel('Rescaled growth rate (r)')\n",
    "ax2.set_ylabel('Rescaled carrying capacity (k)')\n",
    "\n",
    "ax1.contourf(xx, yy, ll, cmap='Blues', alpha=0.8, extent=[xmin, xmax, ymin, ymax])\n",
    "ax1.contour(xx, yy, ll, colors='k')\n",
    "ax2.contourf(xx, yy, ll_emu, cmap='Oranges', alpha=0.8, extent=[xmin, xmax, ymin, ymax])\n",
    "ax2.contour(xx, yy, ll_emu, colors='k')\n",
    "plt.show()\n",
    "\n",
    "fig.savefig(\"results/contours.png\", bbox_inches='tight', dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create grid of parameters\n",
    "xmin, xmax = 0.675, 0.825\n",
    "ymin, ymax = 0.9, 1.1\n",
    "xx, yy = np.mgrid[xmin:xmax:100j, ymin:ymax:100j]\n",
    "params = [list(n) for n in zip(xx, yy)]\n",
    "ll = np.apply_along_axis(log_likelihood, 1, params)\n",
    "ll_emu = np.apply_along_axis(emu, 1, params)\n",
    "ll_emu = [list(e[0][0]) for e in ll_emu]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,7))\n",
    "ax1.title.set_text('Log-Likelihood')\n",
    "ax2.title.set_text('Neural Network')\n",
    "ax1.set_xlabel('Rescaled growth rate (r)')\n",
    "ax1.set_ylabel('Rescaled carrying capacity (k)')\n",
    "ax2.set_xlabel('Rescaled growth rate (r)')\n",
    "ax2.set_ylabel('Rescaled carrying capacity (k)')\n",
    "\n",
    "ax1.contourf(xx, yy, ll, cmap='Blues', alpha=0.8, extent=[xmin, xmax, ymin, ymax])\n",
    "ax1.contour(xx, yy, ll, colors='k')\n",
    "ax2.contourf(xx, yy, ll_emu, cmap='Oranges', alpha=0.8, extent=[xmin, xmax, ymin, ymax])\n",
    "ax2.contour(xx, yy, ll_emu, colors='k')\n",
    "plt.show()\n",
    "\n",
    "fig.savefig(\"results/contours-closer.png\", bbox_inches='tight', dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create grid of parameters\n",
    "xmin, xmax = 0.7125, 0.7875\n",
    "ymin, ymax = 0.95, 1.05\n",
    "xx, yy = np.mgrid[xmin:xmax:100j, ymin:ymax:100j]\n",
    "params = [list(n) for n in zip(xx, yy)]\n",
    "ll = np.apply_along_axis(log_likelihood, 1, params)\n",
    "ll_emu = np.apply_along_axis(emu, 1, params)\n",
    "ll_emu = [list(e[0][0]) for e in ll_emu]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,7))\n",
    "ax1.title.set_text('Log-Likelihood')\n",
    "ax2.title.set_text('Neural Network')\n",
    "ax1.set_xlabel('Rescaled growth rate (r)')\n",
    "ax1.set_ylabel('Rescaled carrying capacity (k)')\n",
    "ax2.set_xlabel('Rescaled growth rate (r)')\n",
    "ax2.set_ylabel('Rescaled carrying capacity (k)')\n",
    "\n",
    "ax1.contourf(xx, yy, ll, cmap='Blues', alpha=0.8, extent=[xmin, xmax, ymin, ymax])\n",
    "ax1.contour(xx, yy, ll, colors='k')\n",
    "ax2.contourf(xx, yy, ll_emu, cmap='Oranges', alpha=0.8, extent=[xmin, xmax, ymin, ymax])\n",
    "ax2.contour(xx, yy, ll_emu, colors='k')\n",
    "plt.show()\n",
    "\n",
    "fig.savefig(\"results/contours-closest.png\", bbox_inches='tight', dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(context='notebook', style='whitegrid', palette=\"deep\", font='Times New Roman', \n",
    "        font_scale=1.5, color_codes=True, rc={\"grid.linewidth\": 1})\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,7))\n",
    "ax1.title.set_text('Log-likelihood contours with MCMC samples')\n",
    "ax2.title.set_text('Neural network contours with MCMC samples') \n",
    "ax1.set_xlabel('Rescaled growth rate (r)')\n",
    "ax1.set_ylabel('Rescaled carrying capacity (k)')\n",
    "ax2.set_xlabel('Rescaled growth rate (r)')\n",
    "ax2.set_ylabel('Rescaled carrying capacity (k)')\n",
    "\n",
    "# Create grid of parameters\n",
    "x = [p[0] for p in chain_emu_rescaled]\n",
    "y = [p[1] for p in chain_emu_rescaled]\n",
    "xmin, xmax = np.min(x), np.max(x)\n",
    "ymin, ymax = np.min(y), np.max(y)\n",
    "xx, yy = np.mgrid[xmin:xmax:100j, ymin:ymax:100j]\n",
    "params = [list(n) for n in zip(xx, yy)]\n",
    "ll = np.apply_along_axis(log_likelihood, 1, params)\n",
    "ll_emu = np.apply_along_axis(emu, 1, params)\n",
    "ll_emu = [list(e[0][0]) for e in ll_emu]\n",
    "\n",
    "# Sort according to differences in log-likelihood\n",
    "idx = diffs.argsort()\n",
    "x_sorted = np.array(x)[idx]\n",
    "y_sorted = np.array(y)[idx]\n",
    "diffs_sorted = diffs[idx]\n",
    "\n",
    "# Add contour lines of log-likelihood\n",
    "ax1.contourf(xx, yy, ll, cmap='Greys', extent=[xmin, xmax, ymin, ymax])\n",
    "#ax1.contour(xx, yy, ll, colors='w')\n",
    "\n",
    "# Plot chain_emu    \n",
    "ax1.set_xlim([xmin, xmax])                                                                           \n",
    "ax1.set_ylim([ymin, ymax])\n",
    "im1 = ax1.scatter(x_sorted, y_sorted, c=diffs_sorted, s=70, edgecolor='k', cmap=\"RdYlGn_r\")\n",
    "\n",
    "# Add contour lines of emulated likelihood\n",
    "ax2.contourf(xx, yy, ll_emu, cmap='Greys', extent=[xmin, xmax, ymin, ymax])\n",
    "#ax2.contour(xx, yy, ll_emu, colors='w')\n",
    "\n",
    "# Plot chain_emu    \n",
    "ax2.set_xlim([xmin, xmax])                                                                           \n",
    "ax2.set_ylim([ymin, ymax])\n",
    "im2 = ax2.scatter(x_sorted, y_sorted, c=diffs_sorted, s=70, edgecolor='k', cmap=\"RdYlGn_r\")\n",
    "\n",
    "fig.colorbar(im1, ax=ax1)\n",
    "fig.colorbar(im2, ax=ax2)\n",
    "plt.show()\n",
    "fig.savefig(\"results/errors-on-contours-best-nn-6-64.png\", bbox_inches='tight', dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing NN performance to 2-step MCMC performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(context='notebook', style='whitegrid', palette=\"deep\", font='Times New Roman', \n",
    "        font_scale=1.5, color_codes=True, rc={\"grid.linewidth\": 1})\n",
    "\n",
    "fig, axes = pints.plot.histogram([metropolis_chain, chain_emu, emulated_chain], \n",
    "                                ref_parameters=true_parameters, \n",
    "                                sample_names=[\"MCMC\", \"Emulator\", \"2-Step MCMC\"],\n",
    "                                parameter_names=[\"Growth rate (r)\", \"Maximum capacity (k)\"])\n",
    "fig.set_size_inches(14, 10)\n",
    "plt.subplots_adjust(wspace=0, hspace=0.4)\n",
    "plt.show()\n",
    "fig.savefig(\"results/log-posterior-samples-best-nn-6-64.png\", bbox_inches='tight', dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(context='paper', style='whitegrid', palette=\"deep\", font='Times New Roman', \n",
    "        font_scale=1.5, color_codes=True, rc={\"grid.linewidth\": 1})\n",
    "fig, axes = pints.plot.trace(metropolis_chains, ref_parameters=true_parameters,\n",
    "                             parameter_names=[\"Growth rate (r)\", \"Maximum capacity (k)\"])\n",
    "fig.set_size_inches(12, 8)\n",
    "plt.subplots_adjust(wspace=0.2, hspace=0.3)\n",
    "plt.show()\n",
    "fig.savefig(\"results/traces-chainmcmc-best-nn-6-64.png\", bbox_inches='tight', dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(context='paper', style='whitegrid', palette=\"deep\", font='Times New Roman', \n",
    "        font_scale=1.5, color_codes=True, rc={\"grid.linewidth\": 1})\n",
    "fig, axes = pints.plot.trace(chains_emu, ref_parameters=true_parameters,\n",
    "                             parameter_names=[\"Growth rate (r)\", \"Maximum capacity (k)\"])\n",
    "fig.set_size_inches(12, 8)\n",
    "plt.subplots_adjust(wspace=0.2, hspace=0.3)\n",
    "plt.show()\n",
    "fig.savefig(\"results/traces-chainemu-best-nn-6-64.png\", bbox_inches='tight', dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(context='paper', style='whitegrid', palette=\"deep\", font='Times New Roman', \n",
    "        font_scale=1.5, color_codes=True, rc={\"grid.linewidth\": 1})\n",
    "fig, axes = pints.plot.trace(emulated_chains, ref_parameters=true_parameters, \n",
    "                             parameter_names=[\"Growth rate (r)\", \"Maximum capacity (k)\"])\n",
    "fig.set_size_inches(12, 8)\n",
    "plt.subplots_adjust(wspace=0.2, hspace=0.3)\n",
    "plt.show()\n",
    "fig.savefig(\"results/traces-emuchain-best-nn-6-64.png\", bbox_inches='tight', dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "metropolis_chain_r = np.array([sample[0] for sample in metropolis_chain])\n",
    "metropolis_chain_k = np.array([sample[1] for sample in metropolis_chain])\n",
    "chain_emu_r = np.array([sample[0] for sample in chain_emu])\n",
    "chain_emu_k = np.array([sample[1] for sample in chain_emu])\n",
    "emulated_chain_r = np.array([sample[0] for sample in emulated_chain])\n",
    "emulated_chain_k = np.array([sample[1] for sample in emulated_chain])\n",
    "\n",
    "w_distance1_r = stats.wasserstein_distance(metropolis_chain_r, chain_emu_r)\n",
    "w_distance1_k = stats.wasserstein_distance(metropolis_chain_k, chain_emu_k)\n",
    "w_distance2_r = stats.wasserstein_distance(metropolis_chain_r, emulated_chain_r)\n",
    "w_distance2_k = stats.wasserstein_distance(metropolis_chain_k, emulated_chain_k)\n",
    "\n",
    "print(\"NN vs MCMC:\", w_distance1_r, w_distance1_k)\n",
    "print(\"2-step MCMC vs MCMC:\", w_distance2_r, w_distance2_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ess = pints.effective_sample_size(metropolis_chain)\n",
    "ess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ess1 = pints.effective_sample_size(chain_emu)\n",
    "ess1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ess2 = pints.effective_sample_size(emulated_chain)\n",
    "ess2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
